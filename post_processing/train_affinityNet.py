import torch
import torch.nn as nn
import torch.optim as optim
import json
import cv2
import numpy as np
import sys
import importlib
import os, argparse
import torch.nn.functional as F
from torchvision import transforms, models
from torch.utils.data import Dataset, DataLoader
from torch.utils.data import random_split
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
import time
from matplotlib import pyplot as plt

# Get the absolute path of the project root
project_root = os.path.abspath(os.path.dirname(__file__) + "/../..")

# Add the project root to sys.path
sys.path.append(project_root)
from lib.dataset.AffinityDataset import *
from lib.dataset.PseudoDataset import *
from lib.dataset.SmokeDataset import *
from lib.dataset.WeaklyDataset import *
from lib.dataset.cropDataset import *
from lib.network.AffinityNet_resnet import *
from lib.network.backbone import choose_backbone
from lib.network.AffinityNet import *
from lib.utils.splitdataset import *
from lib.utils.transform import *
from lib.network import *
from lib.loss.loss import *
from post_processing.inference import *
from lib.utils.metrics import *
from lib.utils.saliencymap import *
from PIL import Image
from lib.utils.pseudo_label import *
from lib.utils.augmentation import *
from lib.utils.image_mask_visualize import *


def parse_args():
    parser = argparse.ArgumentParser(description="Supervised learning")
    # dataset
    parser.add_argument("--json_path", type=str, default=os.path.join(project_root,
                                                                      "smoke-segmentation.v5i.coco-segmentation/test/_annotations.coco.json"),
                        help="Path to COCO annotations JSON file")
    parser.add_argument("--image_folder", type=str,
                        default=os.path.join(project_root, "smoke-segmentation.v5i.coco-segmentation/test/"),
                        help="Path to the image dataset folder")

    parser.add_argument("--non_smoke_image_folder", type=str, default=os.path.join(project_root, "lib/dataset/frames/"),
                        help="Path to the non-smoke image dataset folder")

    parser.add_argument("--save_model_path", type=str,
                        default=os.path.join(project_root, "model/model_classification.pth"),
                        help="Path to save the trained model")

    parser.add_argument("--save_pseudo_labels_path", type=str,
                        default=os.path.join(project_root, "result/pseudo_labels"),

                        help="Path to save the pseudo labels")

    parser.add_argument("--pseudo_labels_path", type=str,
                        default=os.path.join(project_root, "result/pseudo_labels"),
                        help="Path to the pseudo labels generated by classification.py")

    parser.add_argument("--save_cam_path", type=str,
                        default=os.path.join(project_root, "result/vit_s_GradCAM_0.3_3_crf_pseudo_labels"),
                        help="Path to save the cam")
    # parser.add_argument("--save_cam_path", type=str, default=os.path.join(project_root, "final_model/vit_s_GradCAM_0.3_3_pseudo_labels_kd"),
    #                     help="Path to save the cam")

    parser.add_argument("--save_visualization_path", type=str,
                        default=os.path.join(project_root, "result/visualization"),
                        help="Path to save the cam")

    parser.add_argument("--smoke5k", type=bool, default=False, help="use smoke5k or not")
    parser.add_argument("--smoke5k_path", type=str, default=os.path.join(project_root, "SMOKE5K/train/"),
                        help="path to smoke5k")

    parser.add_argument("--Rise", type=bool, default=False, help="use Rise non-smoke or not")
    parser.add_argument("--Rise_path", type=str, default=os.path.join(project_root, "Rise/Strong_negative_frames/"),
                        help="path to Rise")

    parser.add_argument("--crop_smoke_image_folder", type=str,
                        default=os.path.join(project_root, "smoke-segmentation.v5i.coco-segmentation/cropped_images/"),
                        help="Path to the cropped smoke image dataset folder")

    parser.add_argument("--crop_mask_folder", type=str,
                        default=os.path.join(project_root, "smoke-segmentation.v5i.coco-segmentation/cropped_masks/"),
                        help="Path to the cropped image dataset mask folder")

    parser.add_argument("--crop_non_smoke_folder", type=str,
                        default=os.path.join(project_root,
                                             "smoke-segmentation.v5i.coco-segmentation/non_smoke_images/"),
                        help="Path to the cropped image dataset mask folder")

    parser.add_argument("--use_crop", type=bool, default=True, help="use cropped image or not")
    parser.add_argument("--batch_size", type=int, default=8, help="training batch size")
    parser.add_argument("--lr", type=float, default=1e-4, help="learning rate")
    parser.add_argument("--num_epochs", type=int, default=10, help="epoch number")
    parser.add_argument("--img_size", type=int, default=512, help="the size of image")
    parser.add_argument("--num_class", type=int, default=1, help="the number of classes")

    parser.add_argument("--crop_size", default=512, type=int)

    parser.add_argument("--weights_path", required=False, type=str)

    parser.add_argument("--CAM_type", type=str, default='GradCAM',
                        choices=['grad', 'TransCAM', 'TsCAM'],
                        help="CAM type")

    parser.add_argument("--backbone", type=str, default="AffinityNet",
                        help="choose backone")

    parser.add_argument('--manual_seed', default=1, type=int, help='Manually set random seed')

    parser.add_argument('--threshold', default=0.3, type=float, help='Threshold for CAM')

    parser.add_argument("--use_blending", type=bool, default=True, help="blend different threshold cam ")

    return parser.parse_args()


if __name__ == "__main__":
    print("Starting training AffinityNet...")
    args = parse_args()
    print(vars(args))

    print(torch.cuda.is_available())
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    image_transform, mask_transform = get_transforms(args.img_size)
    # set random seed
    torch.manual_seed(args.manual_seed)
    torch.cuda.manual_seed(args.manual_seed)
    np.random.seed(args.manual_seed)
    random.seed(args.manual_seed)

    train_dataset = PseudocamDataset(
        args.crop_smoke_image_folder,
        args.save_cam_path,
        args.crop_mask_folder,
        transform=image_transform,
        mask_transform=mask_transform,
        affinity=True
    )

    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)

    model = choose_backbone(args.backbone)
    model = model.to(device)
    model.train()
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=args.lr)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)

    avg_meter = AverageMeter('loss', 'bg_loss', 'fg_loss', 'neg_loss')
    aff_label_extractor = ExtractAffinityLabelInRadius(cropsize=32, radius=5)

    if args.backbone == 'AffinityNet':
        for epoch in range(1, (args.num_epochs + 1)):
            avg_meter.pop()

            for batch_idx, (images, labels, _, cams, masks) in enumerate(train_loader):
                images, labels = images.to(device), labels.float().to(device)

                cam = cams.to(device)
                loss_total = 0

                for i in range(images.size(0)):
                    img = images[i].unsqueeze(0)  # (1, C, H, W)
                    cam_np = cams[i].cpu().numpy()  # (H, W)
                    # print("cam_np shape",cam_np.shape)#cam_np shape (512, 512)

                    # cam_np: (512, 512)
                    cam_tensor = torch.from_numpy(cam_np).unsqueeze(0)  # Shape: (1, 1, 512, 512)

                    # Downsample to (64, 64)
                    cam_downsampled = F.interpolate(cam_tensor, size=(32, 32), mode='bilinear', align_corners=False)

                    cam_np_down = cam_downsampled.squeeze(0).squeeze(0).cpu().numpy()  # Shape: (64, 64)

                    pseudo_label = np.zeros_like(cam_np_down, dtype=np.uint8)
                    pseudo_label[
                        (cam_np_down < args.threshold) & (cam_np_down >= 0.0001)] = 255  # ignore low confidence
                    pseudo_label[cam_np_down >= args.threshold] = 1  # foreground

                    # if args.blending:
                    #     pseudo_label[cam_np_down>0.7]=1

                    # blended_label=np.zeros_like(cam_np_down, dtype=np.uint8)
                    # blended_label[(cam_np_down < 0.7) & (cam_np_down >= 0.05)]= 255  # ignore low confidence
                    # blended_label[cam_np_down >=0.7] = 1

                    pred_aff = model(img)

                    # Extract affinity labels
                    bg_label, fg_label, neg_label = aff_label_extractor(pseudo_label)
                    bg_label = bg_label.to(device)
                    fg_label = fg_label.to(device)
                    neg_label = neg_label.to(device)

                    bg_count = torch.sum(bg_label) + 1e-5
                    fg_count = torch.sum(fg_label) + 1e-5
                    neg_count = torch.sum(neg_label) + 1e-5

                    loss_bg = torch.sum(-bg_label * torch.log(pred_aff + 1e-5)) / bg_count
                    loss_fg = torch.sum(-fg_label * torch.log(pred_aff + 1e-5)) / fg_count
                    loss_neg = torch.sum(-neg_label * torch.log(1 - pred_aff + 1e-5)) / neg_count

                    loss = loss_bg / 4 + loss_fg / 4 + loss_neg / 2
                    loss_total += loss

                loss_total = loss_total / images.size(0)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                # Update AverageMeter with loss and accuracy
                avg_meter.add({
                    'loss': loss.item(),
                    'bg_loss': loss_bg.item(),
                    'fg_loss': loss_fg.item(),
                    'neg_loss': loss_neg.item()

                })

            scheduler.step()
            avg_loss = avg_meter.get('loss')
            print(f"Epoch [{epoch}/{args.num_epochs}], Loss: {avg_loss:.4f}")

        save_path = os.path.join(
            os.path.dirname(args.save_model_path),
            f"{args.backbone}_{args.CAM_type}_{args.num_epochs}_{os.path.basename(args.save_model_path)}"
        )
        torch.save(model.state_dict(), save_path)
        print("Training complete! Model saved.")


