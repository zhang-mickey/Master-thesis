from turtle import forward
import torch
import torch.nn as nn
import torch.optim as optim
import json
import cv2
import numpy as np
import sys
import importlib
import os, argparse
import torch.nn.functional as F
from torchvision import transforms, models
from torch.utils.data import Dataset, DataLoader
from torch.utils.data import random_split
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
import time
from matplotlib import pyplot as plt

# Get the absolute path of the project root
project_root = os.path.abspath(os.path.dirname(__file__) + "/../..")

# Add the project root to sys.path
sys.path.append(project_root)
from lib.dataset.AffinityDataset import *
from lib.dataset.PseudoDataset import *
from lib.dataset.SmokeDataset import *
from lib.dataset.WeaklyDataset import *
from lib.dataset.cropDataset import *
from lib.network.AffinityNet_resnet import *
from lib.network.AffinityNet import *
from lib.network.backbone import choose_backbone
from lib.utils.splitdataset import *
from lib.utils.transform import *
from lib.network import *
from lib.loss.loss import *
from post_processing.inference import *
from lib.utils.metrics import *
from lib.utils.saliencymap import *
from PIL import Image
from lib.utils.pseudo_label import *
from lib.utils.augmentation import *
from lib.utils.image_mask_visualize import *


def parse_args():
    parser = argparse.ArgumentParser(description="Supervised learning")
    # dataset
    parser.add_argument("--json_path", type=str, default=os.path.join(project_root,
                                                                      "smoke-segmentation.v5i.coco-segmentation/test/_annotations.coco.json"),
                        help="Path to COCO annotations JSON file")

    parser.add_argument("--image_folder", type=str,
                        default=os.path.join(project_root, "smoke-segmentation.v5i.coco-segmentation/test/"),
                        help="Path to the image dataset folder")

    parser.add_argument("--non_smoke_image_folder", type=str, default=os.path.join(project_root, "lib/dataset/frames/"),
                        help="Path to the non-smoke image dataset folder")

    parser.add_argument("--save_model_path", type=str,
                        default=os.path.join(project_root, "model/model_classification.pth"),
                        help="Path to save the trained model")

    parser.add_argument("--save_pseudo_labels_path", type=str,
                        default=os.path.join(project_root, "result/pseudo_labels"),

                        help="Path to save the pseudo labels")

    parser.add_argument("--pseudo_labels_path", type=str,
                        default=os.path.join(project_root, "result/pseudo_labels"),
                        help="Path to the pseudo labels generated by classification.py")

    # parser.add_argument("--save_cam_path", type=str, default=os.path.join(project_root, "result/resnet101_GradCAM_10_pseudo_labels"),
    #                     help="Path to save the cam")

    parser.add_argument("--save_cam_path", type=str,
                        default=os.path.join(project_root, "final_model/vit_s_GradCAM_0.3_3_pseudo_labels_kd"),
                        help="Path to save the cam")

    parser.add_argument("--save_visualization_path", type=str,
                        default=os.path.join(project_root, "result/visualization"),
                        help="Path to save the cam")

    parser.add_argument("--smoke5k", type=bool, default=False, help="use smoke5k or not")
    parser.add_argument("--smoke5k_path", type=str, default=os.path.join(project_root, "SMOKE5K/train/"),
                        help="path to smoke5k")

    parser.add_argument("--Rise", type=bool, default=False, help="use Rise non-smoke or not")
    parser.add_argument("--Rise_path", type=str, default=os.path.join(project_root, "Rise/Strong_negative_frames/"),
                        help="path to Rise")

    parser.add_argument("--crop_smoke_image_folder", type=str,
                        default=os.path.join(project_root, "smoke-segmentation.v5i.coco-segmentation/cropped_images/"),
                        help="Path to the cropped smoke image dataset folder")

    parser.add_argument("--crop_mask_folder", type=str,
                        default=os.path.join(project_root, "smoke-segmentation.v5i.coco-segmentation/cropped_masks/"),
                        help="Path to the cropped image dataset mask folder")

    parser.add_argument("--crop_non_smoke_folder", type=str,
                        default=os.path.join(project_root,
                                             "smoke-segmentation.v5i.coco-segmentation/non_smoke_images/"),
                        help="Path to the cropped image dataset mask folder")

    parser.add_argument("--use_crop", type=bool, default=True, help="use cropped image or not")

    # train
    parser.add_argument("--batch_size", type=int, default=8, help="training batch size")

    parser.add_argument("--lr", type=float, default=1e-4, help="learning rate")

    parser.add_argument("--num_epochs", type=int, default=10, help="epoch number")

    parser.add_argument("--img_size", type=int, default=512, help="the size of image")
    parser.add_argument("--num_class", type=int, default=1, help="the number of classes")

    parser.add_argument("--crop_size", default=512, type=int)

    parser.add_argument("--weights_path", required=False, type=str)

    parser.add_argument("--CAM_type", type=str, default='GradCAM',
                        choices=['grad', 'TransCAM', 'TsCAM'],
                        help="CAM type")
    parser.add_argument("--backbone", type=str, default="AffinityNet",
                        help="choose backone")

    parser.add_argument('--manual_seed', default=1, type=int, help='Manually set random seed')

    parser.add_argument('--threshold', default=0.3, type=float, help='Threshold for CAM')

    parser.add_argument('--logt', default=1, type=int,
                        help='iteration')
    parser.add_argument('--beta', default=16, type=int,
                        help='ite')
    return parser.parse_args()


if __name__ == "__main__":
    print("Starting training AffinityNet...")
    args = parse_args()

    print(vars(args))

    print(torch.cuda.is_available())
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    image_transform, mask_transform = get_transforms(args.img_size)
    # set random seed
    torch.manual_seed(args.manual_seed)
    torch.cuda.manual_seed(args.manual_seed)
    np.random.seed(args.manual_seed)
    random.seed(args.manual_seed)

    train_dataset = PseudocamDataset(
        args.crop_smoke_image_folder,
        args.save_cam_path,
        args.crop_mask_folder,
        transform=image_transform,
        mask_transform=mask_transform,
        affinity=True
    )

    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)

    model = choose_backbone(args.backbone)
    model = model.to(device)
    model.train()
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(model.parameters(), lr=args.lr)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)

    avg_meter = AverageMeter('loss')
    # max_batches = 2
    # 512//8
    aff_label_extractor = ExtractAffinityLabelInRadius(cropsize=32, radius=5)

    if args.backbone == 'AffinityNet':
        save_path = os.path.join(
            os.path.dirname(args.save_model_path),
            f"{args.backbone}_{args.CAM_type}_{args.num_epochs}_{os.path.basename(args.save_model_path)}"
        )
        model.load_state_dict(torch.load(save_path))
        model.eval()
        model.cuda()

        iou_cam_sum = 0.0
        iou_aff_sum = 0.0
        total_samples = 0

        for batch_idx, (images, labels, image_ids, cams, masks) in enumerate(train_loader):
            orig_img_size = images.shape[2:]

            with torch.no_grad():
                for i, (img, label, img_id, cam, mask) in enumerate(zip(images, labels, image_ids, cams, masks)):
                    # cam torch.Size([1, 512, 512])
                    cam_np = cam.cpu().numpy()
                    pseudo_label_cam = (cam_np >= args.threshold).astype(np.float32)

                    cam_tensor = torch.from_numpy(cam_np).unsqueeze(0)  # Shape: (1, 1, 512, 512)

                    cam_downsampled = F.interpolate(cam_tensor, size=(32, 32), mode='bilinear', align_corners=False)

                    cam_np_down = cam_downsampled.squeeze(0).squeeze(0).cpu().numpy()  # Shape: (64, 64)

                    affinity_matrix = torch.pow(model.forward(img.unsqueeze(0).cuda(), True), args.beta)

                    # print(f"affinity_matrix shape: {affinity_matrix.shape}")
                    # affinity_matrix shape: torch.Size([1, 34, 672])

                    trans_matrix = affinity_matrix / torch.sum(affinity_matrix, dim=0, keepdim=True)
                    # print(f"trans_matrix shape: {trans_matrix.shape}")                  

                    for _ in range(args.logt):
                        trans_matrix = torch.matmul(trans_matrix, trans_matrix)

                    cam_tensor_down = torch.from_numpy(cam_np_down).float().to(device)  # [32, 32]

                    cam_vec = cam_tensor_down.view(1, -1)  # [1, 1024]

                    # Perform propagation
                    refined_cam_vec = torch.matmul(cam_vec, trans_matrix)  # shape: (1, 1024)
                    refined_cam = refined_cam_vec.view(1, 1, 32, 32)

                    refined_cam_up = F.interpolate(refined_cam, size=(512, 512), mode='bilinear', align_corners=False)

                    refined_np = refined_cam_up.squeeze().cpu().numpy()
                    # refined_np = refined_np - np.min(refined_np)
                    # refined_np = refined_np / (np.max(refined_np) + 1e-5)
                    pseudo_label_aff = np.zeros_like(refined_np, dtype=np.uint8)
                    pseudo_label_aff[refined_np >= args.threshold] = 1

                    # Visualization

                    mean = np.array([0.485, 0.456, 0.406])
                    std = np.array([0.229, 0.224, 0.225])  # Adjust based on your dataset

                    img_np = img.cpu().numpy().transpose(1, 2, 0)  # CHW -> HWC
                    img_np = std * img_np + mean  # Denormalize
                    img_np = np.clip(img_np, 0, 1)

                    # pseudo_label = (cam > args.threshold).astype(np.float32)      
                    # pseudo_label = pseudo_label.detach().cpu().numpy()

                    if isinstance(pseudo_label_aff, torch.Tensor):
                        # For PyTorch tensor
                        pseudo_label_aff = (pseudo_label_aff > args.threshold).float()
                    else:
                        # For NumPy array
                        pseudo_label_aff = (pseudo_label_aff > args.threshold).astype(np.uint8)

                    # print("type pf pseudo-label",type(pseudo_label))
                    # type pf pseudo-label <class 'torch.Tensor'>
                    gt_mask = mask.squeeze().cpu().numpy()
                    gt_mask = (gt_mask > 0.5).astype(np.float32)

                    intersection_aff = np.logical_and(gt_mask, pseudo_label_aff).sum()
                    union_aff = np.logical_or(gt_mask, pseudo_label_aff).sum()
                    iou_aff = intersection_aff / (union_aff + 1e-8)

                    intersection = np.logical_and(gt_mask, pseudo_label_cam).sum()
                    union = np.logical_or(gt_mask, pseudo_label_cam).sum()
                    iou_cam = intersection / (union + 1e-8)

                    iou_cam_sum += iou_cam
                    iou_aff_sum += iou_aff
                    total_samples += 1
                    if i <= 16:
                        fig, ax = plt.subplots(1, 5, figsize=(25, 5))

                        ax[0].imshow(img_np)
                        ax[0].set_title('Original Image')
                        ax[0].axis('off')

                        # CAM visualization
                        ax[1].imshow(refined_np, cmap='jet')
                        ax[1].set_title('Affinity')
                        ax[1].axis('off')

                        # cam_np = (cam_np - cam_np.min()) / (cam_np.max() - cam_np.min() + 1e-8)
                        print(f"CAM min: {cam_np.min()}, max: {cam_np.max()}, mean: {cam_np.mean()}")
                        cam_np = cam_np.squeeze()
                        ax[2].imshow(cam_np, cmap='jet')
                        ax[2].set_title('Class Activation Map')
                        ax[2].axis('off')

                        ax[3].imshow(gt_mask, cmap='gray')
                        ax[3].set_title(f'Ground Truth')
                        ax[3].axis('off')

                        ax[4].imshow(pseudo_label_aff, cmap='gray')
                        ax[4].set_title(f'pseudo_label')
                        ax[4].axis('off')

                        save_dir = os.path.join(args.save_visualization_path, args.backbone, "vit_s")
                        os.makedirs(save_dir, exist_ok=True)

                        plt.tight_layout()
                        plt.savefig(os.path.join(save_dir, f'visualization_{img_id}.png'), bbox_inches='tight')
                        plt.close()

                    save_pseudo_labels_path = os.path.join(
                        os.path.dirname(args.save_pseudo_labels_path),
                        f"{args.backbone}_{args.CAM_type}_{args.threshold}_{args.num_epochs}_{os.path.basename(args.save_pseudo_labels_path)}"
                    )
                    # np.save(
                    #     os.path.join(save_pseudo_labels_path, f"affinityNet_{img_id}.npy"),
                    #     refined_np
                    # )

    print(
        f"\nFinal aff/cam Mean IoU: {iou_aff_sum / total_samples:.4f} {iou_cam_sum / total_samples:.4f}(over {total_samples} samples)")



